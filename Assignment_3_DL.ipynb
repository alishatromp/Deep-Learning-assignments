{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4583cd51",
   "metadata": {},
   "source": [
    "### 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f812e",
   "metadata": {},
   "source": [
    "### This is not recommended, even if its randomly selected. The purpose of weight initialization techniques like He is to break the symmetry and ensure that neurons in the network have different initial values, helps the network learn more effectively. \n",
    "### He initialization, which is designed for ReLU (Rectified Linear Unit) activation functions, initializes weights by sampling from a Gaussian distribution with mean 0 and variance 2/n, where \"n\" is the number of input units to the neuron. This variance is specifically chosen to ensure that the weights are not too small or too large, which can lead to problems like vanishing or exploding gradients during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9343e8",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07c8fb",
   "metadata": {},
   "source": [
    "### 2. Is it OK to initialize the bias terms to 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9cc4b",
   "metadata": {},
   "source": [
    "### Initializing bias terms to 0 is a common practice in neural network initialization, and it is generally considered acceptable. Unlike weight initialization, where you want to break symmetry and provide some randomness to the initial weights, bias terms are not subject to the same concerns. Bias terms are used to shift the activation function, introducing a level of freedom that doesn't affect symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9745e3d",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a52d4a",
   "metadata": {},
   "source": [
    "### 3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57335b36",
   "metadata": {},
   "source": [
    "### 1. SELU has the property of self normalization, which means that when used in specific types of DNN, it can help stablize and maintain a consistent mean and variance throughout the layers. This helps dealing with issues such as vanishing gradient or exploding gradients. \n",
    "\n",
    "### 2. SELU encourages sparsity in NN helping to deal with overfitting. Sparsity means more neurons being inactive compared to RELU.\n",
    "\n",
    "### 3. SELU is smooth, piecewise linear function that is differentiable everywhere. SELU enables the use of gradient based optimization like gradient descent which can lead to more stable and predictable convergence during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78ad4f",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8484f7d",
   "metadata": {},
   "source": [
    "### 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc328b3",
   "metadata": {},
   "source": [
    "### The use of activation functions depends on the problem, architecture, and data characteristics. \n",
    "\n",
    "1. SELU\n",
    "- Use SELU - when working with deep neural networks, especially deep feedforward networks or recurrent networks like LSTMS.\n",
    "- SELU can help address the vanishing gradient problem and encourage self normalization in deep networks, leading to faster convergence.\n",
    "\n",
    "2. Leaky RELU\n",
    "- use leaky RELU when you want to mitigate the 'dying RELU' problem where neurons can become inactive and not update their weights during training.\n",
    "- parametric RELU and randomized Leaky RELU can be considered when you want to learn the leaky slope or introduce randomness, respectively to improve training.\n",
    "\n",
    "3. RELU\n",
    "- Use RELU when you want a simple, computationally efficient activation function that is effective in most cases.\n",
    "\n",
    "4. tanh\n",
    "- Use tanh when you need an activation function that squashes input values between -1 and 1, which helps with zero centered activations.\n",
    "\n",
    "5. Logistic(Sigmoid)\n",
    "- Use this AF when you need binary classification output (0 or 1).\n",
    "- It is commonly used in the output layer of binary classifiers or networks designed for probability estimation. \n",
    "\n",
    "6. Softmax\n",
    "- Use softmax in the output layer when you have a multiclass classification problem. \n",
    "- it converts raw scores or logits into probability distributions over multiple classes.\n",
    "- It is used in conjunction with cross-entropy loss for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9a87b",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09854fb4",
   "metadata": {},
   "source": [
    "### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993212b",
   "metadata": {},
   "source": [
    "### The momentum parameter is a technique used to accelerate convergence and stabilize training, but when its set too high, it can have several unintended consequences.\n",
    "\n",
    "1. Overshooting the minima - high momentum values will continue moving in a particular direction even when it should be slowing down or changing direction. As a result, the optimizer may overshoot the minimum of the loss func, causing training to oscillate or diverge.\n",
    "2. Difficulty converging\n",
    "3. Difficulting escaping local minima\n",
    "4. sensitivity to learning rate\n",
    "5. loss function fluctuations\n",
    "\n",
    "To avoid these issue choose a reasonable momentum value, typically in the range of 0.5 to 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdce72",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852bf58",
   "metadata": {},
   "source": [
    "### 6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f7e16",
   "metadata": {},
   "source": [
    "Producing a sparse model means reducing the number of parameters or connections in a neural network.\n",
    "\n",
    "1. Pruning - removes less important weights or connections from a trained neural network.\n",
    "2. Quantization - involves reducing the precision of weights and activations in a neural network, typically from a 32 bit floating point values to lower bit represenations.\n",
    "3. Knowledge distillation - is a technique in which a large, complex model (teacher) transfers its knowledge to a smaller, simpler model (student).\n",
    "\n",
    "These methods can be used individually or in combination to produce sparse models that are suitable for deployment on resource-constrained devices or for reducing the computational cost of large neural networks while preserving their performance to a reasonable extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c52f2",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca27f4",
   "metadata": {},
   "source": [
    "### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cad28d",
   "metadata": {},
   "source": [
    "### Dropout is a regularization technique commonly used in neural networks to prevent overfitting. It involves randomly deactivating (dropping out) a fraction of neurons during each training iteration. While dropout can introduce some additional computational overhead during training, it typically does not slow down inference (making predictions on new instances).\n",
    "\n",
    "### During training, dropout randomly sets a fraction of neuron activations to zero. This introduces a form of stochasticity and encourages the network to be more robust by preventing it from relying too heavily on any single neuron or feature.\n",
    "\n",
    "### MC (Monte Carlo) Dropout is a technique that involves running inference with dropout applied multiple times (usually several forward passes) and averaging the predictions to obtain more reliable uncertainty estimates.\n",
    "\n",
    "### While MC Dropout does introduce some additional computational cost during inference because you perform multiple forward passes, it can provide valuable uncertainty estimates for the model's predictions, which can be useful in applications like Bayesian deep learning or active learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff99b73",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f80e0",
   "metadata": {},
   "source": [
    "### 8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "### a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "\n",
    "### b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#load and unpack the CIFAR-10 dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "#Normalize pixel values to between 0 and 1\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "#Build the deep neural network\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(32, 32,3))) #add the input layer\n",
    "model.add(tf.keras.layers.Flatten()) #flatten layer to input images\n",
    "\n",
    "for _ in range (20):\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    \n",
    "    model.add(Dense(10, activation = 'softmax')) #output layer with 10 neurons\n",
    "    model.compile(optimizer=Nadam(),loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "return model\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5818a5",
   "metadata": {},
   "source": [
    "### c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_with_batch_norm():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(32, 32, 3)))  # Input layer\n",
    "    model.add(tf.keras.layers.Flatten())  # Flatten layer to flatten input images\n",
    "    \n",
    "    # Add 20 hidden layers with He initialization, ELU activation, and Batch Normalization\n",
    "    for _ in range(20):\n",
    "        model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "        model.add(BatchNormalization()) #add batchnormalization\n",
    "        model.add(Activation('elu'))\n",
    "    \n",
    "    # Output layer with softmax activation for 10 classes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train the model with Batch Normalization\n",
    "dnn_model_with_batch_norm = build_dnn_with_batch_norm()\n",
    "history_with_batch_norm = dnn_model_with_batch_norm.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f653a",
   "metadata": {},
   "source": [
    "### Yes converges faster for sure, training speed is also fast. Producing a better model depends on many factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ccc0d",
   "metadata": {},
   "source": [
    "### d. Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba804d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the build_dnn function to use SELU activation\n",
    "def build_dnn_with_selu():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(32, 32, 3)))  # Input layer\n",
    "    model.add(tf.keras.layers.Flatten())  # Flatten layer to flatten input images\n",
    "    \n",
    "    # Add 20 hidden layers with LeCun initialization and SELU activation\n",
    "    for _ in range(20):\n",
    "        model.add(Dense(100, kernel_initializer='lecun_normal', activation='selu'))\n",
    "    \n",
    "    # Output layer with softmax activation for 10 classes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train the model with SELU activation\n",
    "# tried with lesser epochs...100 epochs took lots of time.\n",
    "\n",
    "dnn_model_with_selu = build_dnn_with_selu()\n",
    "history_with_selu = dnn_model_with_selu.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7282181",
   "metadata": {},
   "source": [
    "### e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d26bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AlphaDropout\n",
    "\n",
    "# Modify the build_dnn function to include AlphaDropout\n",
    "def build_dnn_with_alpha_dropout():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(32, 32, 3)))  # Input layer\n",
    "    model.add(tf.keras.layers.Flatten())  # Flatten layer to flatten input images\n",
    "    \n",
    "    # Add 20 hidden layers with He initialization, ELU activation, Batch Normalization, and AlphaDropout\n",
    "    for _ in range(20):\n",
    "        model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('elu'))\n",
    "        model.add(AlphaDropout(0.2))  # Adjust dropout rate as needed\n",
    "    \n",
    "    # Output layer with softmax activation for 10 classes\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train the model with AlphaDropout\n",
    "dnn_model_with_alpha_dropout = build_dnn_with_alpha_dropout()\n",
    "history_with_alpha_dropout = dnn_model_with_alpha_dropout.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Apply MC Dropout for predictions (example)\n",
    "mc_predictions = [dnn_model_with_alpha_dropout.predict(X_test) for _ in range(100)]\n",
    "mc_pred_mean = np.mean(mc_predictions, axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
