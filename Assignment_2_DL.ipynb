{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960b8240",
   "metadata": {},
   "source": [
    "### 1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df3b21",
   "metadata": {},
   "source": [
    "An artificial neuron is also called a perceptron.\n",
    "\n",
    "It consists of several main components:\n",
    "\n",
    "Inputs: Neurons receive inputs from other neurons or directly from the input data. Each input is multiplied by a weight, and the weighted inputs are used in subsequent computations.\n",
    "\n",
    "Weights: Each input is associated with a weight, which determines the strength of the input's contribution to the neuron's output. These weights are learnable parameters that are adjusted during the training process.\n",
    "\n",
    "Summation Function: The weighted inputs are summed up, often with an additional bias term. The bias helps control the neuron's threshold for activation.\n",
    "\n",
    "Activation Function: The summed value (including the bias) is passed through an activation function. This function introduces non-linearity to the neuron, allowing it to model complex relationships in the data.\n",
    "\n",
    "Output: The output of the activation function becomes the neuron's output, which can be passed as an input to other neurons in subsequent layers or used as the final output of the neural network.\n",
    "\n",
    "Both biological and artificial neurons rely on inputs. A biological neuron is triggered by a stimulus which generates a synapse. The intensity of which is measured by its weights and sends feedback to its neighbouring cell which is the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9dd129",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e300e5",
   "metadata": {},
   "source": [
    "### 2. What are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c35af",
   "metadata": {},
   "source": [
    "1. Sigmoid Activation (Logistic Activation):\n",
    "The sigmoid function takes any input and maps it to a value between 0 and 1. \n",
    "\n",
    "The sigmoid function was historically popular, but it has some limitations, such as vanishing gradients for very large or very small inputs. This can lead to slow convergence during training, especially in deep networks.\n",
    "\n",
    "2. Hyperbolic Tangent (Tanh) Activation:\n",
    "\n",
    "The hyperbolic tangent function is similar to the sigmoid but maps inputs to values between -1 and 1.\n",
    "Tanh overcomes the issue of the sigmoid's output range being restricted to positive values, but it still suffers from vanishing gradients for very large or very small inputs.\n",
    "\n",
    "3. Rectified Linear Unit (ReLU):\n",
    "The ReLU activation function is a piecewise linear function that outputs the input for positive values and 0 for negative values:\n",
    "\n",
    "ReLU(x)=max(0,x)\n",
    "\n",
    "ReLU helps address the vanishing gradient problem and has become one of the most widely used activation functions due to its simplicity and effectiveness. However, it can suffer from the \"dying ReLU\" problem, where neurons can become inactive during training and never recover.\n",
    "\n",
    "ReLu also comes with \n",
    "1. Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient for negative inputs to prevent dying ReLU neurons.\n",
    "2. PReLU is similar to Leaky ReLU but allows the slope of the negative part to be learned during training.\n",
    "3. ELU is another variation of ReLU that reduces the dying ReLU problem and produces negative outputs for negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb9bfc",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25965142",
   "metadata": {},
   "source": [
    "### 3(1). Explain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "\n",
    "Rosenblatt's model tried to mimic the functioning of a biological neuron and to perform binary classification tasks. The perceptron model paved the way for the development of more complex neural networks.\n",
    "\n",
    "The perceptron consists of the following components:\n",
    "\n",
    "Inputs: A set of input features (also called input neurons) denoted as x1, x2, x3...xn\n",
    "Weights: Each input is associated with a weight w1, w2, w3 which represents the strength of the connection between the input and the perceptron.\n",
    "\n",
    "Summation Function: The weighted inputs are summed up, often with an additional bias term b.\n",
    "\n",
    "Activation Function: The output of the summation function is passed through an activation function (often a step function) to produce the final output of the perceptron.\n",
    "\n",
    "Output: The final outcome or predicted values to be fed into other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a735a",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902eb0c0",
   "metadata": {},
   "source": [
    "### 3(2(1)). Use a simple perceptron with weights w 0 , w 1 , and w 2  as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e50ef",
   "metadata": {},
   "source": [
    "Let's say the threshold (θ) is 0\n",
    "\n",
    "Weighted sum \n",
    "\n",
    "z = w 0 + w1.x1 + w2.x2 (x1,x2 are co-ods of data points)\n",
    "\n",
    "If z>=0 than classify DP as 1. \n",
    "If z<0 than classify DP as -1.\n",
    "\n",
    "so:\n",
    "(3,4) = -1 + 2.3 + 1.4 = -1 + 6 + 4 = 9\n",
    "Since 9>= 0, so classify this as 1.\n",
    "(5,2) = 1\n",
    "(1,-3) is -1.\n",
    "(-8, -3) is -1.\n",
    "(-3,0) is -1.\n",
    "\n",
    "(3, 4) is classified as 1\n",
    "(5, 2) is classified as 1\n",
    "(1, -3) is classified as -1\n",
    "(-8, -3) is classified as -1\n",
    "(-3, 0) is classified as -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c8fd9",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563ebd3",
   "metadata": {},
   "source": [
    "### 3(2(2)). Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159a187",
   "metadata": {},
   "source": [
    "A multi-layer perceptron (MLP) with hidden layers and non-linear activation functions can solve the XOR problem and a wide range of other complex problems by learning non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed25e44",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2810de",
   "metadata": {},
   "source": [
    "### 3(3). What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7294943",
   "metadata": {},
   "source": [
    "An Artificial Neural Network (ANN) is a computational model inspired by the structure and functioning of biological neural networks in the human brain.\n",
    "\n",
    "Some salient highlights are.\n",
    "1. FNNs\n",
    "2. CNNs\n",
    "3. RNNs.\n",
    "4. GANs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecefb10e",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a469dbe",
   "metadata": {},
   "source": [
    "### 4. Explain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037c467",
   "metadata": {},
   "source": [
    "The learning process of an Artificial Neural Network (ANN) involves adjusting the synaptic weights (also known as connection weights) between neurons to enable the network to make accurate predictions or classifications. This process is typically achieved through a training phase, where the network learns from a labeled dataset. The most common learning algorithms for ANNs include supervised learning, unsupervised learning, and reinforcement learning. \n",
    "\n",
    "The main challenge in assigning synaptic weights lies in finding the right values that allow the neural network to learn effectively. If the weights are initialized incorrectly or updated too aggressively, the network may fail to converge to a good solution. This can result in slow training, divergence, or getting stuck in local minima of the loss function.\n",
    "\n",
    "For example:\n",
    "\n",
    "we have y = x^2.\n",
    "we initialize the w1 =0.2, w2= -0.5.\n",
    "So for (x=2) we have \n",
    "z = w1.x+w2\n",
    "z = 0.2*2 -0.5 = -0.1\n",
    "The predicted o/p is -0.1 where actual should be 4. The error would be high and BP would try to adjust the weights to reduce this error.\n",
    "\n",
    "Best to use appropriate activating functions to address this. Including regularization and batch normalization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a583",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a9a30",
   "metadata": {},
   "source": [
    "### 5. Explain, in details, the backpropagation algorithm. What are the limitations of this algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70130de",
   "metadata": {},
   "source": [
    "BP - It enables ANNs to learn from data by adjusting the synaptic weights to minimize the error or loss function. \n",
    "\n",
    "The key step in backpropagation involves computing the gradients of the loss with respect to the weights and biases. This step is performed layer by layer, starting from the output layer and moving backward through the hidden layers.\n",
    "\n",
    "Some limitations are:\n",
    "\n",
    "1. Vanishing and Exploding gradients\n",
    "2. Local minima\n",
    "3. Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f712c",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5316b72",
   "metadata": {},
   "source": [
    "### 6. Describe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2592f",
   "metadata": {},
   "source": [
    "The process of adjusting these weights involves an optimization algorithm, most commonly gradient descent, and its variants.\n",
    "\n",
    "1. Initialize\n",
    "2. Forward Propagation\n",
    "3. Error calculation\n",
    "4. Back propagation\n",
    "5. Weight update\n",
    "\n",
    "\n",
    "The process of adjusting interconnection weights in a multi-layer neural network can be computationally intensive, and the choice of hyperparameters (e.g., learning rate, batch size, regularization strength) can significantly impact training success.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6cf14",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfe8ef",
   "metadata": {},
   "source": [
    "### 7. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?\n",
    "\n",
    "- This is the core of the weight adjustment process.\n",
    "- Gradients of the loss with respect to the weights are calculated through backpropagation.\n",
    "- The gradients represent the rate of change of the loss concerning each weight, indicating how much each weight should be adjusted to minimize the error.\n",
    "- The backpropagation process occurs layer by layer, starting from the output layer and moving backward to the input layer.\n",
    "- For each layer, the gradients are computed for both the weights and biases, using the chain rule to propagate errors from the next layer backward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8cfb1",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5714c0c",
   "metadata": {},
   "source": [
    "### 8. Write short notes on:\n",
    "\n",
    "1. Artificial neuron\n",
    "- An artificial neuron, also known as a perceptron, is the basic computational unit in artificial neural networks (ANNs).\n",
    "\n",
    "2. Multi-layer perceptron\n",
    "-  is a type of feedforward artificial neural network with three or more layers: an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "3. Deep learning\n",
    "- is a type of feedforward artificial neural network with three or more layers: an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "4. Learning rate\n",
    "- is a hyperparameter in machine learning and deep learning that controls the size of weight updates during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1562a",
   "metadata": {},
   "source": [
    "### 2. Write the difference between:-\n",
    "\n",
    "1. Activation function vs threshold function\n",
    "\n",
    "Activation Function: Activation functions introduce non-linearity into neural networks and help them model complex relationships in data. Common activation functions include sigmoid, ReLU, and tanh.\n",
    "\n",
    "Threshold Function: The threshold function is a simple activation function used in binary classification. It outputs 1 if the input exceeds a certain threshold and 0 otherwise. It is not differentiable and less commonly used in modern neural networks.\n",
    "\n",
    "\n",
    "2. Step function vs sigmoid function\n",
    "\n",
    "Step Function: The step function is a basic activation function that outputs 1 for inputs greater than or equal to zero and 0 otherwise. It is not differentiable and rarely used in deep learning.\n",
    "\n",
    "Sigmoid Function: The sigmoid function is a popular activation function that smoothly transforms inputs to the range (0, 1). It is differentiable and used in various applications, especially in the output layer for binary classification.\n",
    "\n",
    "\n",
    "3. Single layer vs multi-layer perceptron\n",
    "\n",
    "Single Layer Perceptron:\n",
    "\n",
    "- Has only one layer of artificial neurons.\n",
    "- Suitable for linearly separable problems.\n",
    "- Cannot model complex relationships.\n",
    "- Primarily used for binary classification.\n",
    "\n",
    "\n",
    "Multi-layer Perceptron (MLP):\n",
    "- Consists of multiple layers, including at least one hidden layer.\n",
    "- Can model non-linear relationships.\n",
    "- Suitable for a wide range of tasks, including regression and complex classification.\n",
    "- A powerful architecture used in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
